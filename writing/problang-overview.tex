\documentclass{sp}

% The \pdf* commands provide metadata for the PDF output.
% Do not use LaTeX style / commands like \emph{} inside these.
\pdfauthor{Author Full Name(s)}
\pdftitle{Full title}
\pdfkeywords{XXX, XXX}

% Optional short title inside square brackets, for the running headers.
% If no short title is given, no title appears in the headers.
%\title[Adjective ordering preferences]{On the grammatical source of adjective ordering preferences%
\title[Practical introduction to RSA]{A practical introduction to the Rational Speech Act modeling framework%
	\thanks{We thank XXX.}}

% Optional short author inside square brackets, for the running headers.
% If no short author is given, no authors print in the headers.
\author[]{% As many authors as you like, each separated by \AND. %%% uncomment to de-anonymize
%	\spauthor{author1 \\ \institute{affiliation1}} \AND
%	\spauthor{author2 \\ \institute{affiliation2}} \AND
%	\spauthor{author3 \\ \institute{affiliation3}}
}

\usepackage{linguex}
\usepackage{qtree}
\qtreecenterfalse
\usepackage{tree-dvips}
\usepackage{phonetic}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{lineno}
\usepackage{graphicx}
%\usepackage{qdotbranch}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{CJK}
\usepackage{tikz}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{hyperref}

\def\url#1{\expandafter\string\csname #1\endcsname}

\newcommand{\gcs}[1]{\textcolor{blue}{[gcs: #1]}}  
\newcommand{\mf}[1]{\textcolor{orange}{[mf: #1]}}  

\newcommand{\type}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\lam}{\ensuremath{\lambda}}
\newcommand{\sem}[1]{\ensuremath{[\![#1]\!]}}

\renewcommand{\firstrefdash}{}

\begin{document}

\maketitle

\begin{abstract}
	Recent advances in computational cognitive science (i.e., simulation-based probabilistic programs) have paved the way for significant progress in formal, implementable models of pragmatics. Rather than describing a pragmatic reasoning process, these models articulate and implement one, deriving both qualitative and quantitative predictions of human behavior---predictions that consistently prove correct, demonstrating the viability and value of the framework. The current paper provides a practical introduction to the Bayesian Rational Speech Act modeling framework, serving as a companion piece to the hands-on web-book at \href{https://www.problang.org}{www.problang.org}. After providing a conceptual overview of the framework, we then walk readers through the resources of the web-book.
\end{abstract}

\begin{keywords}
	XXX, XXX
\end{keywords}

\tableofcontents

\section{Introduction}

Much work in formal, compositional semantics follows the tradition of positing systematic but inflexible theories of meaning. However, in practice, the meanings we derive from language are heavily dependent on nearly all aspects of context, both linguistic and situational. To formally explain these nuanced aspects of meaning and better understand the compositional mechanism that delivers them, recent work in formal pragmatics recognizes semantics not as one of the final steps in meaning calculation, but rather as one of the first. Within the Bayesian Rational Speech Act (RSA) framework \citep{goodmanfrank2016,frankejaeger2016}, speakers and listeners reason about each other's reasoning about the literal interpretation of utterances. The resulting interpretation necessarily depends on the literal interpretation of an utterance, but is not necessarily wholly determined by it. This move---reasoning about likely interpretations---provides ready explanations for complex phenomena ranging from metaphor \citep{kaoetal2014metaphor} and hyperbole \citep{kaoetal2014} to the specification of thresholds in degree semantics \citep{lassitergoodman2013}.

The probabilistic pragmatics approach leverages the tools of structured probabilistic models formalized in a stochastic $\lambda$-calculus to develop and refine a general theory of communication. The framework synthesizes the knowledge and approaches from diverse areas---formal semantics, Bayesian models of inference, formal theories of measurement, philosophy of language, etc.---into an articulated theory of language in practice. These new tools yield broader empirical coverage and richer explanations for linguistic phenomena through the recognition of language as a means of communication, not merely a vacuum-sealed formal system. By subjecting the heretofore off-limits land of pragmatics to articulated formal models, the rapidly growing body of research both informs pragmatic phenomena and enriches theories of linguistic meaning.

These models are particularly well-suited for capturing XXX Goodies list of RSA

The current paper offers a practical introduction to the modeling framework, serving as a companion piece to the hands-on web-book at \href{https://www.problang.org}{www.problang.org}. We begin in Section \ref{overview} with a high-level overview of RSA, walking through its basic implementation and the philosophical foundations that informed the architectural choices. We then explore variations to the basic architecture in Section \ref{variations}, surveying technological innovations that have allowed for broader empirical coverage. Section \ref{practicalities} discusses common practical considerations faced by the working modeler. In Section \ref{limitations}, we explore limitations of the current framework, which also serve as guidance for future extensions. Section \ref{summary} concludes.


\section{High-level overview of RSA} \label{overview}

The RSA framework views language understanding as a process of recursive social reasoning between speakers and listeners: listeners interpret the utterances they hear by reasoning about how speakers generate them; speakers choose their utterances by reasoning about how listeners interpret them. In the basic, vanilla RSA model from \cite{frankgoodman2012}, this recursion involves three layers of inference. Typically formulated as statements of conditional probability, as in (\ref{L0}), (\ref{S1}), and (\ref{L1}), these inference layers correspond to models of speakers and listeners.

\begin{equation} \label{L0}
P_{L_0}(s \mid u) \propto \sem{u}(s)
\end{equation}
\begin{equation} \label{U}
U_{S_1}(u; s) = \textrm{log}P_{L_0}(s \mid u) - C(u)
\end{equation}
\begin{equation} \label{S1}
P_{S_1}(u \mid s) \propto \textrm{exp}(\alpha \cdot U_{S_1}(u;s))
\end{equation}
\begin{equation} \label{L1}
P_{L_1}(s \mid u) \propto P_{S_1}(u \mid s) \cdot P(s)
\end{equation}

The reasoning grounds out in the naive, literal listener, $L_0$, who interprets utterances
according to their literal semantics. The semantics of utterance $u$ is captured in meaning
function $\sem{u} \colon s \mapsto [0;1]$ which maps states to truth values. We assume binary
truth-values here, but the formalism works just as well for non-binary, e.g., fuzzy semantics
\mf{insert refs: Degen on overspecification, Steinert on vagueness, ...?}. According to
\eqref{L0}, $L_0$ hears some utterance $u$ and infers the state of the world $s$ that $u$ was
meant to describe by restricting the set of possible states to just those that are compatible
with the literal, truth-functional semantics of $u$, returning a uniform probability
distribution over the states $s$ that $u$ maps to \texttt{true}.

One layer up, a pragmatic speaker, $S_1$, chooses utterances in proportion to their utility $U_{S_{1}}$. Utterances are useful to the extent that they maximize the probability that $L_0$ will infer the correct $s$ on the basis of $u$, while minimizing the cost of $u$ ($C(u)$; speakers aim to be efficient). So, when selecting utterances, $S_1$ considers their effect on interpretation (i.e., on $L_0$'s resulting beliefs); utterances that are most likely to lead $L_0$ to the correct belief are most likely to be chosen by $S_1$.

At the top layer of inference, the pragmatic listener, $L_1$, interprets utterances to infer the true state of the world. However, unlike $L_0$, who reasons directly about the utterance semantics, $L_1$ reasons instead about the process that generated the utterance; that process is the speaker $S_1$. $L_1$ thus infers $s$ on the basis of $u$ by reasoning about the probability that $S_1$ would have chosen $u$ to signal $s$ to $L_0$; the higher that probability, the more likely $L_1$ is to conclude that $S_1$ indeed intended to communicate $s$. Because $L_1$ reasons about $S_1$, who in turn reasons about the literal semantics in $L_0$, $L_1$'s interpretation is affected by the semantics of $u$, albeit only indirectly via the $S_1$ layer. This space between the semantics (i.e., $L_0$) and the resulting interpretation (i.e., the posterior beliefs of $L_1$) is where pragmatics enters. To see how, it would help to consider a concrete communication scenario.

In its initial formulation, \cite{frankgoodman2012} used the RSA framework to model referent choices in efficient communication. Suppose we are in a world as in Figure \ref{ref-game} with three objects: a blue square, a blue circle, and a green square. Suppose further that a speaker is trying to signal a single object in this world to a cooperative listener, and that the speaker can only use a single-word utterance to do so. The utterances available to the speaker include ``blue'', ``green'', ``square'', and ``circle''; the possible states the listener might infer correspond to the three objects: blue-square, blue-circle, and green-square. We have the expected truth-functional semantics for the utterances: ``blue'' maps blue-square and blue-circle to true but green-square to false, ``green'' maps blue-square and blue-circle to false but green-square to true, etc.

\begin{figure}[t]
\centering
\includegraphics[width=3in]{rsa-scene.pdf}
\caption{An example referential-communication game from \cite{frankgoodman2012}.}
\label{ref-game}
\end{figure}

With the semantics as stated, $L_0$ interprets utterances to return uniform probability distributions over the compatible states. Hearing the utterance ``blue'', $L_0$ returns a belief distribution over states that divides probability equally between blue-square and blue-circle, the only objects compatible with the semantics of ``blue''. Hearing ``circle'', $L_0$ returns a distribution with 100\% of the probability on blue-circle---$L_0$ is certain that the speaker intends to signal blue-circle. With $L_0$ formulated in this way, we have an agent who interprets utterances naively, according to the literal semantics. 

$S_1$ chooses utterances by simulating their effect on $L_0$. Suppose the speaker wants to communicate blue-circle to $L_0$. Two utterances, ``green'' and ``square'', stand no chance of communicating the intended state to $L_0$ and so they are ruled out. The other two utterances, ``blue'' and ``circle'', are both literally compatible with blue-circle, but one of the utterances is more likely to lead $L_0$ to the correct belief state. If the speaker were to utter ``blue'', we saw that $L_0$'s belief distribution would be evenly split between blue-square and blue-circle. In other words, ``blue'' has a 50\% chance of leading $L_0$ to the correct state. On the other hand, ``circle'' has a 100\% chance of leading $L_0$ to the correct state. Assuming equal utterance costs, ``circle'' is thus twice as useful to the speaker as ``blue'' for communicating blue-circle to $L_0$, and $S_1$ reflects this asymmetry in utility by assigning twice as much probability to ``circle'' in the posterior distribution over utterance choices.\footnote{With no scaling. XXX} For communicating the blue-square state, both ``blue'' and ``square'' have a 50\% chance of leading $L_0$ to the correct state, so both utterances are equally likely and thus equally probable in the $S_1$ posterior.

Within this simple reference-game scenario, $L_1$ reasons pragmatically about $S_1$ to break the symmetry in the semantics. Hearing ``blue'', $L_1$ will invert the $S_1$ model to determine which state (i.e., which object) the speaker is most likely trying to communicate. Had the speaker wanted to communicate the blue-circle state, we saw above that the speaker would have been more likely to utter ``circle''. But the speaker did not utter ``circle''; she uttered ``blue'' instead. This counterfactual reasoning leads $L_1$ to down-weight the probability of the state that ``circle'' could have uniquely picked out, since the speaker could have said ``circle'' but chose not to. As a result, more probability gets assigned to the blue-square state, the other state compatible with the semantics of the utterance. In this way, we capture the Gricean specificity implicature associated with uttering ``blue'' in a scenario as in Figure \ref{ref-game}: the speaker probably intends the blue square because if she wanted to communicate the blue circle she could have said ``circle''.

A key component of the RSA framework is the updating of prior beliefs: listeners use the utterances they hear as signal with which to update their beliefs about the world. In walking through the reference-game example above, we assumed a uniform prior over world states: before hearing the speaker's utterance, $L_1$ had no reason to suspect that any object was more or less likely to get referenced. With a different, more informative prior, it is possible to shift the qualitative predictions of $L_1$. Suppose that $L_1$ has reason to suspect that the blue-circle state is most likely to get referenced. Now there are two opposing pressures operating on $L_1$'s interpretation of the utterance ``blue" (i.e., on $L_1$'s posterior beliefs): the prior belief favoring blue-circle over blue-square and the specificity implicature discussed above that favors blue-square over blue-circle (if the speaker had wanted to reference blue-circle, she could have said ``circle''). With a sufficiently strong prior in favor of blue-circle, it is possible to override the specificity implicature so that $L_1$ interprets ``blue'' as confirming his suspicions and referring to blue-circle.

XXX scalar implicature?

\mf{start MF - - - - - }

\paragraph{Relation to Gricean Maxims.} The vanilla RSA model, as described above in Equations
\eqref{L0}--\eqref{L1}, is a direct formalization of Grice's idea that listeners can infer
pragmatic meaning based on the assumption that speakers adheres to certain rules of behavior,
the so-called Maxims of Conversation \citep{Grice1975:Logic-and-Conve}. This becomes
transparent if we rewrite the speaker's utterance choice probability, starting from the
definition in \eqref{S1}.
%
\begin{align} \label{eq:S1-rewrite}
  P_{S_1(u\mid s)} & \propto \exp \left [ \alpha \left (\log P_{L_0}(s \mid u) - C(u) \right)  \right ] & \text{\textcolor{gray}{[definitions \eqref{U} \& \eqref{S1}]}} \\
  % & = \left [\exp \left ( \log P_{L_0}(s \mid u) - C(u) \right ) \right ]^\alpha & \text{\textcolor{gray}{[rules exponential function]}} \nonumber \\  
  & = \left (P_{L_0}(s \mid u) \  \frac{1}{\log C(u)} \right)^{\alpha} & \text{\textcolor{gray}{[rules exponential \& log]}} \nonumber \\
  & = \left ( \frac{\sem{u}(s)}{| \sem{u} |} \ \frac{1}{\log C(u)} \right )^{\alpha} & \text{\textcolor{gray}{[definition \eqref{L0}]}} \nonumber 
\end{align}
%
The reformulation in~\eqref{eq:S1-rewrite} shows how the speaker's choice probabilities are a
product of three factors, each of which corresponds to a central postulate of Grice's
concerning how cooperative and, arguably, rational speakers should tailor their contributions
to a conversation. Let's just recast everything in more telling notation by setting:
$\text{Truth}(u,s) = \sem{u}(s)$, $\text{Informativity}(u) = |\sem{u}|^{-1}$, and
$\text{Economy(u)} = \log C(u)$. We can then rewrite the speaker rule as (notice that for
binary truth-values, $\alpha$ can be dropped from the factor $\text{Truth}$):
\begin{align} \label{eq:S1-three-factor-formulation}
  P_{S_1(u\mid s)}   & \propto \text{Truth}(u,s) \ \text{Informativity}(u)^{\alpha} \ \text{Economy}(u)^{\alpha} % \propto \underbrace{\sem{u}(s)^{\alpha}}_{\text{Quantity}} \ \underbrace{| \sem{u} |^{-\alpha}}_{\text{Quantity}} \  \underbrace{(\log C(u))^{-\alpha}}_{\text{Manner}} & \text{\textcolor{gray}{[definition \eqref{L0}]}} \nonumber
\end{align}
These three factors directly capture (one formalization of) the Gricean Maxims of Quality,
Quantity, and Manner. In effect, the RSA speaker assumes that speakers (i) maximize truth
(i.e., for classical binary truth-values, utter no falsehoods if they can select at least one
true message), (ii) maximize informativity (all else equal) and (iii) maximize utterance
economy (all else equal). The more rational a speaker (i.e., the higher $\alpha$), the more
pronounced are the latter two optimization tendencies. (Notice that this reflects the special
status that Grice bestowed on the Maxim of Quantity: (binary) truthfulness is not affected by
the speaker's degree of optimizing informativity and costs.)

The formulation in \eqref{eq:S1-rewrite} also highlights that it is easy to define nearby
alternative models of speaker production, which go beyond Grice's seminal work, but allow to
capture psychological factors, e.g., from a more algorithmic perspective on how speakers
generate utterances. For instance, some applications might want to capture differences between
utterance alternatives that are not related to production economy or cost, but rather to the
ease with which an alternative utterance comes to mind. The latter factor of \emph{differential
  salience} is arguably not subject to optimization: it is not the case that the more rational
a speaker is, the more she would tend to select only the utterances which come to mind most
easily. We can implement differential salience (ease of retrieval) for instance in a
formulation like this:
%
\begin{align}
  \label{eq:S1-utterance-salience}
  P_{S_1(u\mid s)}^{\text{Salience}}   & \propto \text{Truth}(u,s) \ \text{Informativity}(u)^{\alpha} \ \text{Salience}(u)
\end{align}
An implementation of the speaker model in \eqref{eq:S1-utterance-salience} would implement the
factor $\text{Salience}(u)$ as an \emph{utterance prior}, suggesting an algorithmic
picture of utterance choice: the speaker searches for utterances to choose, based on a gradient
of how easy these utterances come to mind, and then compares the available options (weighted by
their salience/prominence) based on the other factors relevant for communication: truth and
informativity. \mf{I'm not sure if this paragraph needs to go here; it might have a better
  place elsewhere; but stressing the 'algorithmic picture' somewhere might be good}

\paragraph{Informativity from alignment of beliefs.} As we have just seen, the vanilla RSA
model's definition of speaker's choice probabilities can be interpreted as a product of three
factors that correspond closely to Gricean Maxims of Quality, Quantity and Manner. Clearly,
this is just one formal implementation; others are readily conceivable. Given the particular
importance of informativity for many pragmatic explanations, it is justified to further dissect
the way informativity is treated in the RSA model. In the following, we show that \eqref{U} \&
\eqref{S1} follows from a simple idea, when spelled out in a natural way: the speaker has some
belief about the relevant world states, e.g., captured in probability distribution $P_{S\text{-}Bel}$;
after an utterance of $u$ the literal listener's beliefs about the relevant world states are
represented as another probability distribution $P_{LL\text{-}Bel}(u)$; an utterance $u$ is better
than another utterance $u'$ if the listener's beliefs $P_{LL\text{-}Bel}(u)$ after hearing $u$ are
more aligned with the speaker's beliefs $P_{S\text{-}Bel}$ than the listener's beliefs
$P_{LL\text{-}Bel}(u')$ after hearing $u'$. If we spell out ``being more aligned with'' in terms of an
information-theoretic notion of divergence between probability distributions, the vanilla RSA
model is recovered. In the back of it all, however, is simply the idea that the speaker prefers
utterances that best align speaker's and listener's relevant beliefs about the world.

A useful notion of ``alignment of probability distributions'' is the information-theoretic
measure of Kullback-Leibler divergence. KL-divergence is not symmetric, but considers one of
two to-be-compared probability distributions as the ground-truth or the objective function to
to be approximated. This makes sense in for application to language because it is the speaker's
beliefs that the listener should align to. KL-divergence then measures divergence in terms of,
essentially, the expected number of extra bits needed to encode a signal that was generated
from the true distribution with the approximate distribution. \mf{leave out that last
  sentence?} The Kullback-Leibler divergence between (baseline/true) probability distribution
$P_{S\text{-}Bel}$ and (approximate/to-be-optimized) probability distribution
$P_{LL\text{-}Bel}(u)$ is defined as:
\begin{align}
  \label{eq:KL-divergence}
  \text{KL}(P_{S\text{-}Bel} \mid \mid P_{LL\text{-}Bel}(u)) = - \sum_{s} P_{S\text{-}Bel}(s) \ \log \frac{P_{S\text{-}Bel}(s)}{P_{LL\text{-}Bel}(u,s)}
\end{align}

Using KL-divergence, we can then state a more general definition of utterance utilities, to
replace \eqref{U}:
\begin{align}
  \label{eq:Utils-KL-based}
  U_{S_1}(u; s) = \text{KL}(P_{S\text{-}Bel} \mid \mid P_{LL\text{-}Bel}(u)) - C(u)
\end{align}
The formulation in \eqref{U} is a special case of the more general \eqref{eq:Utils-KL-based}
which follows if the speaker's beliefs about the relevant world states are degenerate, i.e.,
whenever the speaker knows the true world state $s^{*}$, so that $P_{S\text{-}Bel}(s^{*})=1$.
In that case, we have:
\begin{align*}
  \text{KL}(P_{S\text{-}Bel} \mid \mid P_{LL\text{-}Bel}(u)) & = - \sum_{s} P_{S\text{-}Bel}(s) \ \log \frac{P_{S\text{-}Bel}(s)}{P_{LL\text{-}Bel}(u,s)} \\
  & =  - \log\frac{1}{P_{LL\text{-}Bel}(s^*)} = \log P_{LL\text{-}Bel}(s^*)
\end{align*}



If the speaker knows the true world
state $s^*$, she has a degenerate probabilistic belief about world states $P_{S_1} \in
\Delta(S)$ which assigns probability 1 to the true $s^*$ and 0 to any other world state.
After an utterance $u$, the literal listener also has a probability distribution over world
states $P_{L_0} \in \Delta(S)$. One way of thinking about what happens in cooperative
discourse that maximizes relevant information flow is that the speaker tries to choose
utterances $u$ such that the listener's beliefs (after hearing an utterance) is maximally
similar to the belief of the speaker. In other words, speakers choose to say things that
assimilate the listener's belief state to their own as much as possible. A notion of divergence
between probability distributions is the Kullback-Leibler divergence. A defintion of utility in
terms of minimization of KL-divergence derives the original suprisal-based definition, if the
speaker's beliefs are degenerate:

$U_{S_1}(P_{S_1}, P_{L_0}) = - \text{KL}(P_{S_1} \mid\mid P_{L_0} )$

$= - \sum_{s'} P_{S_1}(s) \ \log \frac{P_{S_1}(s)}{P_{L_0}(s)}$

$$ = - \log\frac{1}{P_{L_0}(s^*)} = \log P_{L_0}(s^*) $$





\begin{itemize}
\item derivation from KL divergence
\end{itemize}

\mf{end MF - - - - - }

\section{Variations on vanilla} \label{variations}

We have seen how vanilla RSA can model pragmatic reasoning in simple reference games and specificity implicatures more generally. In this section, we explore recent extensions of RSA meant to handle more complex language phenomena. We organize our discussion around deviations from vanilla RSA, highlighting novel technology and the phenomena it captures.

$$P_{S_1}(u|s) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s|\sem{u}) - C(u)))$$

\subsection{Lexical inference type models}

write like this, useful for this, have the following goodies..

$$P_{S_1}(u|s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s|\sem{u}^{\textbf{x}}) - C(u)))$$


\subsection{QUD inference}

$$P_{S_1}(u|s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(F(s,\textbf{x})|\sem{u}) - C(u)))$$

\subsection{Context/Prior inference}

$$P_{S_1}(u|s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s|\sem{u}, \textbf{x}) - C(u)))$$

\subsection{Epistemic inference}

\subsection{Complex utility/utilty inference}

\section{Modeling practicalities} \label{practicalities}

Free parameters (optimality, cost, alternatives)

World priors: Both how to model and how to measure

QUD

Linking functions

\section{Extensions/limitations} \label{limitations}

Incrementality / processing

Compositionality (CCG chapter in dippl?)

NLP (Dan Klein)

Individual variability (cite Franke \& Degen)

``Just'' a computational level theory. Doesn't bear on mechanisms. But, potentially a hook via resource rational analysis (Leider, Griffiths, ...)

Hand-coded: Don't have a theory of alternatives, state priors

\section{Summary and outlook} \label{summary}

XXX


\bibliography{problang}

%\begin{addresses} %%% uncomment to de-anonymize
%	\begin{address}
%		author1
%		\email{email1}
%	\end{address}
%	\begin{address}
%		author2
%		\email{email2}
%	\end{address}
%	\begin{address}
%		author3
%		\email{email3}
%	\end{address}
%\end{addresses}



\end{document}
