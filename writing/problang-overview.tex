\documentclass{sp}

% The \pdf* commands provide metadata for the PDF output.
% Do not use LaTeX style / commands like \emph{} inside these.
\pdfauthor{Author Full Name(s)}
\pdftitle{Full title}
\pdfkeywords{XXX, XXX}

% Optional short title inside square brackets, for the running headers.
% If no short title is given, no title appears in the headers.
%\title[Adjective ordering preferences]{On the grammatical source of adjective ordering preferences%
\title[Practical introduction to RSA]{A practical introduction to the Rational Speech Act modeling framework%
	\thanks{We thank XXX.}}

% Optional short author inside square brackets, for the running headers.
% If no short author is given, no authors print in the headers.
\author[]{% As many authors as you like, each separated by \AND. %%% uncomment to de-anonymize
%	\spauthor{author1 \\ \institute{affiliation1}} \AND
%	\spauthor{author2 \\ \institute{affiliation2}} \AND
%	\spauthor{author3 \\ \institute{affiliation3}}
}

\usepackage{linguex}
\usepackage{qtree}
\qtreecenterfalse
\usepackage{tree-dvips}
\usepackage{phonetic}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{lineno}
\usepackage{graphicx}
%\usepackage{qdotbranch}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{CJK}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}

\def\url#1{\expandafter\string\csname #1\endcsname}

\newcommand{\gcs}[1]{\textcolor{blue}{[gcs: #1]}}
\newcommand{\mf}[1]{\textcolor{orange}{[mf: #1]}}
\newcommand{\mht}[1]{\textcolor{purple}{[mht: #1]}}

\newcommand{\type}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\newcommand{\lam}{\ensuremath{\lambda}}
\newcommand{\sem}[1]{\ensuremath{[\![#1]\!]}}

\renewcommand{\firstrefdash}{}

\begin{document}

\maketitle

\begin{abstract}
	Recent advances in computational cognitive science (i.e., simulation-based probabilistic programs) have paved the way for significant progress in formal, implementable models of pragmatics. Rather than describing a pragmatic reasoning process, these models formalize and implement one, deriving both qualitative and quantitative predictions of human behavior---predictions that consistently prove correct, demonstrating the viability and value of the framework. The current paper provides a practical introduction to the Bayesian Rational Speech Act modeling framework, serving as a companion piece to the hands-on web-book at \href{https://www.problang.org}{www.problang.org}. After providing a conceptual overview of the framework, we then walk readers through the resources of the web-book.
\end{abstract}

\begin{keywords}
	XXX, XXX
\end{keywords}

\tableofcontents

\section{Introduction}

Much work in formal, compositional semantics follows the tradition of positing systematic but inflexible theories of meaning. In practice, however, the meanings listeners derive from language are heavily dependent on nearly all aspects of context, both linguistic and situational. To formally explain these nuanced aspects of meaning and better understand the compositional mechanism that delivers them, recent work in formal pragmatics recognizes semantics not as one of the final steps in meaning calculation, but rather as one of the first. Within the Bayesian Rational Speech Act (RSA) framework \citep{goodmanfrank2016,frankejaeger2016}, speakers and listeners reason about each other's reasoning about the literal interpretation of utterances. The resulting interpretation necessarily depends on the literal interpretation of an utterance, but is not necessarily wholly determined by it. This move---reasoning about likely interpretations---provides ready explanations for complex phenomena ranging from metaphor \citep{kaoetal2014metaphor} and hyperbole \citep{kaoetal2014} to the specification of thresholds in degree semantics \citep{lassitergoodman2013}.

The probabilistic pragmatics approach leverages the tools of structured probabilistic models formalized in a stochastic $\lambda$-calculus to develop and refine a general theory of communication. The framework synthesizes the knowledge and approaches from diverse areas---formal semantics, Bayesian models of reasoning under uncertainty, formal theories of measurement, philosophy of language, etc.---into an articulated theory of language in practice. These new tools yield broader \mht{broader than what?} empirical coverage and richer explanations for linguistic phenomena through the recognition of language as a means of communication, not merely a vacuum-sealed formal system. By subjecting the heretofore off-limits land of pragmatics to articulated formal models, the rapidly growing body of research both informs pragmatic phenomena and enriches theories of linguistic meaning.

These models are particularly well-suited for capturing XXX Goodies list of RSA

The current paper offers a practical introduction to the modeling framework, serving as a companion piece to the hands-on web-book at \href{https://www.problang.org}{www.problang.org}. We begin in Section \ref{overview} with a high-level overview of RSA, walking through its basic implementation and the philosophical foundations that informed the architectural choices. We then explore variations to the basic architecture in Section \ref{variations}, surveying technological innovations that have allowed for broader empirical coverage. Section \ref{practicalities} discusses common practical considerations facing the modeler. In Section \ref{limitations}, we explore limitations of the current framework, which also serve as guidance for future extensions. Section \ref{summary} concludes.


\section{High-level overview of RSA} \label{overview}

The RSA framework views language understanding as a process of recursive social reasoning between speakers and listeners: listeners interpret the utterances they hear by reasoning about how speakers generate them; speakers choose their utterances by reasoning about how listeners interpret them. In the basic RSA model from \cite{frankgoodman2012} (hereafter, Vanilla RSA), this recursion involves three layers of inference. Typically formulated as statements of conditional probability, as in (\ref{L0}), (\ref{S1}), and (\ref{L1}), these inference layers correspond to models of speakers and listeners.

\begin{equation} \label{L0}
P_{L_0}(s \mid u) \propto \sem{u}(s)
\end{equation}
\begin{equation} \label{U}
U_{S_1}(u; s) = \textrm{log}P_{L_0}(s \mid u) - C(u)
\end{equation}
\begin{equation} \label{S1}
P_{S_1}(u \mid s) \propto \textrm{exp}(\alpha \cdot U_{S_1}(u;s))
\end{equation}
\begin{equation} \label{L1}
P_{L_1}(s \mid u) \propto P_{S_1}(u \mid s) \cdot P(s)
\end{equation}

The reasoning grounds out in the naive, literal listener, $L_0$, who interprets utterances
according to their literal semantics. The semantics of utterance $u$ is captured in meaning
function $\sem{u} \colon s \mapsto [0;1]$, which maps states to truth values. We assume binary
truth-values here, but the formalism works just as well for non-binary, fuzzy values
\mf{insert refs: Degen on overspecification, Steinert on vagueness, ...?}.

According to \eqref{L0}, $L_0$ hears some utterance $u$ and infers the state of the world $s$ that $u$ was
meant to describe. $L_0$ performs this inference by restricting the set of possible states to just those that are compatible
with the literal, truth-functional semantics of $u$.
Thus, \eqref{L0} returns a uniform probability distribution over the states $s$ that $u$ maps to \texttt{true}.
\mht{note: we are presenting the version of RSA where the literal listener doesn't have $P(s)$... is this a good idea?}

One layer up, a pragmatic speaker, $S_1$, chooses utterances in proportion to their utility $U_{S_{1}}$. Utterances are useful to the extent that they maximize the probability that $L_0$ will infer the correct $s$ on the basis of $u$ (i.e., informativity), while minimizing the cost of $u$ ($C(u)$; speakers aim to be efficient). So, when selecting utterances, $S_1$ considers their effect on interpretation (i.e., on $L_0$'s resulting beliefs); utterances that are most likely to lead $L_0$ to the correct belief are most likely to be chosen by $S_1$.

At the top layer of inference, the pragmatic listener, $L_1$, interprets utterances to infer the true state of the world. However, unlike $L_0$, who reasons directly about the utterance semantics, $L_1$ reasons instead about the process that generated the utterance; that process is the speaker $S_1$. $L_1$ thus infers $s$ on the basis of $u$ by reasoning about the probability that $S_1$ would have chosen $u$ to signal $s$ to $L_0$; the higher that probability, the more likely $L_1$ is to conclude that $S_1$ indeed intended to communicate $s$. Because $L_1$ reasons about $S_1$, who in turn reasons about the literal semantics in $L_0$, $L_1$'s interpretation is affected by the semantics of $u$, albeit only indirectly via the $S_1$ layer. This space between the semantics (i.e., $L_0$) and the resulting interpretation (i.e., the posterior beliefs of $L_1$) is where pragmatics enters. \mht{haven't discussed $P(s)$ at this point, see previous note}. To see how, it would help to consider a concrete communication scenario.

In its initial formulation, \cite{frankgoodman2012} used the RSA framework to model referent choices in efficient communication. Suppose we are in a world as in Figure \ref{ref-game} with three objects: a blue square, a blue circle, and a green square. Suppose further that a speaker is trying to signal a single object in this world to a cooperative listener, and that the speaker can only use a single-word utterance to do so. The utterances available to the speaker include ``blue'', ``green'', ``square'', and ``circle''; the possible states the listener might infer correspond to the three objects: blue-square, blue-circle, and green-square. We have the expected truth-functional semantics for the utterances: ``blue'' maps blue-square and blue-circle to true but green-square to false, ``green'' maps blue-square and blue-circle to false but green-square to true, etc.

\begin{figure}[t]
\centering
\includegraphics[width=3in]{rsa-scene.pdf}
\caption{An example referential-communication game from \cite{frankgoodman2012}.}
\label{ref-game}
\end{figure}

With the semantics as stated, $L_0$ interprets utterances to return uniform probability distributions over the compatible states. Hearing the utterance ``blue'', $L_0$ returns a belief distribution over states that divides probability equally between blue-square and blue-circle, the only objects compatible with the semantics of ``blue''. Hearing ``circle'', $L_0$ returns a distribution with 100\% of the probability on blue-circle---$L_0$ is certain that the speaker intends to signal blue-circle. With $L_0$ formulated in this way, we have an agent who interprets utterances naively, according to the literal semantics.

$S_1$ chooses utterances by simulating their effect on $L_0$. Suppose the speaker wants to communicate blue-circle to $L_0$. Two utterances, ``green'' and ``square'', stand no chance of communicating the intended state to $L_0$ and so they are ruled out entirely. The other two utterances, ``blue'' and ``circle'', are both literally compatible with blue-circle, but one of the utterances is more likely to lead $L_0$ to the correct belief state. If the speaker were to utter ``blue'', we saw that $L_0$'s belief distribution would be evenly split between blue-square and blue-circle. In other words, ``blue'' has a 50\% chance of leading $L_0$ to the correct state. On the other hand, ``circle'' has a 100\% chance of leading $L_0$ to the correct state. Assuming equal utterance costs, ``circle'' is thus twice as useful to the speaker as ``blue'' for communicating blue-circle to $L_0$, and $S_1$ reflects this asymmetry in utility by assigning twice as much probability to ``circle'' in the posterior distribution over utterance choices.\footnote{With no scaling. XXX} For communicating the blue-square state, both ``blue'' and ``square'' have a 50\% chance of leading $L_0$ to the correct state, so both utterances are equally likely and thus equally probable in the $S_1$ posterior.
\mht{we may want to have a figure of probability tables, a la MF's probability tables in problang code boxes...}

Within this simple reference-game scenario, $L_1$ reasons pragmatically about $S_1$ to break the symmetry in the semantics. Hearing ``blue'', $L_1$ will invert the $S_1$ model to determine which state (i.e., which object) the speaker is most likely trying to communicate. Had the speaker wanted to communicate the blue-circle state, we saw above that the speaker would have been more likely to utter ``circle''. But the speaker did not utter ``circle''; she uttered ``blue'' instead. This counterfactual reasoning leads $L_1$ to down-weight the probability of the state that ``circle'' could have uniquely picked out, since the speaker could have said ``circle'' but chose not to. As a result, more probability gets assigned to the blue-square state, the other state compatible with the semantics of the utterance. In this way, we capture the Gricean specificity implicature associated with uttering ``blue'' in a scenario as in Figure \ref{ref-game}: the speaker probably intends the blue square because if she wanted to communicate the blue circle she could have said ``circle''.

A key component of the RSA framework is the updating of prior beliefs: listeners use the utterances they hear as signal with which to update their beliefs about the world. In walking through the reference-game example above, we assumed a uniform prior over world states: before hearing the speaker's utterance, $L_1$ had no reason to suspect that any object was more or less likely to get referenced. With a different, more informative prior, it is possible to shift the qualitative predictions of $L_1$. Suppose that $L_1$ has reason to suspect that the blue-circle state is most likely to get referenced. Now there are two opposing pressures operating on $L_1$'s interpretation of the utterance ``blue" (i.e., on $L_1$'s posterior beliefs): the prior belief favoring blue-circle over blue-square and the specificity implicature discussed above that favors blue-square over blue-circle (if the speaker had wanted to reference blue-circle, she could have said ``circle''). With a sufficiently strong prior in favor of blue-circle, it is possible to override the specificity implicature so that $L_1$ interprets ``blue'' as confirming his suspicions and referring to blue-circle.
\mht{it can very easily sound like there is a mathematical rule that explain everything while explaining nothing. do we want to talk about how to empirically ground prior beliefs? or, reiterate that this is a modeling framework, not a particular theory (and thus, different prior beliefs correspond to different theories within the framework?)}\gcs{we mention empirical grounding in the practicalities section}

\paragraph{Relation to Gricean maxims.} The vanilla RSA model, as described above in Equations
\eqref{L0}--\eqref{L1}, is a direct formalization of Grice's idea that listeners can infer
pragmatic meaning based on the assumption that speakers adhere to certain rules of behavior,
the so-called ``Maxims of Conversation'' \citep{Grice1975:Logic-and-Conve}. This relationship to Grice's maxims becomes more
transparent if we rewrite the speaker's utterance-choice probability, starting from the
definition in \eqref{S1}.
%
\begin{align} \label{eq:S1-rewrite}
  P_{S_1(u\mid s)} & \propto \exp \left [ \alpha \left (\log P_{L_0}(s \mid u) - C(u) \right)  \right ] & \text{\textcolor{gray}{[definitions \eqref{U} \& \eqref{S1}]}} \\
  % & = \left [\exp \left ( \log P_{L_0}(s \mid u) - C(u) \right ) \right ]^\alpha & \text{\textcolor{gray}{[rules exponential function]}} \nonumber \\
  & = \left (P_{L_0}(s \mid u) \  \frac{1}{\log C(u)} \right)^{\alpha} & \text{\textcolor{gray}{[rules exponential \& log]}} \nonumber \\
  & = \left ( \frac{\sem{u}(s)}{\mid  \sem{u} \mid } \ \frac{1}{\log C(u)} \right )^{\alpha} & \text{\textcolor{gray}{[definition \eqref{L0}]}} \nonumber
\end{align}
%
The reformulation in~\eqref{eq:S1-rewrite} shows how the speaker's choice probabilities are a
product of three factors, each corresponding to a central postulate concerning how cooperative and, arguably, rational speakers should tailor their contributions
to a conversation. Suppose we recast the components as follows:
$\text{Truth}(u,s) = \sem{u}(s)$, $\text{Informativity}(u) = \mid \sem{u}\mid ^{-1}$, and
$\text{Economy(u)} = \log C(u)$. \gcs{we might explicitly justify this recasting} We can then rewrite the speaker rule as:\footnote{For
binary truth-values, $\alpha$ can be dropped from the factor $\text{Truth}$.}
\begin{align} \label{eq:S1-three-factor-formulation}
  P_{S_1(u\mid s)}   & \propto \text{Truth}(u,s) \ \text{Informativity}(u)^{\alpha} \ \text{Economy}(u)^{\alpha} % \propto \underbrace{\sem{u}(s)^{\alpha}}_{\text{Quantity}} \ \underbrace{\mid  \sem{u} \mid ^{-\alpha}}_{\text{Quantity}} \  \underbrace{(\log C(u))^{-\alpha}}_{\text{Manner}} & \text{\textcolor{gray}{[definition \eqref{L0}]}} \nonumber
\end{align}
These three factors directly capture (a formalization of) the Gricean maxims of Quality,
Quantity, and Manner. In effect, the RSA speaker assumes that speakers (i) maximize truth
(i.e., they utter no falsehoods if they can select at least one
true message), (ii) maximize informativity (all else equal), and (iii) maximize utterance
economy (all else equal). The more rational a speaker (i.e., the higher the value of the scaling parameter $\alpha$), the more
pronounced the latter two optimization tendencies become. This behavior reflects the special
status that Grice bestowed on the Maxim of Quantity: (binary) truthfulness is not affected by
the speaker's degree of optimizing informativity and costs.

The formulation in \eqref{eq:S1-rewrite} highlights the ease with which it is possible to define nearby
alternative models of speaker production, going beyond Grice's seminal work by
capturing additional psychological factors (e.g., from a more algorithmic perspective on how speakers
generate utterances). For instance, some applications might want to capture differences between
utterance alternatives that are not related to production economy or cost, but rather to the
ease with which an alternative utterance comes to mind. The latter factor of \emph{differential
  salience} is arguably not subject to optimization: it is not the case that the more rational
a speaker is, the more she would tend to select only the utterances which come to mind most
easily. We can implement differential salience (ease of retrieval) in a
formulation as follows:
%
\begin{align}
  \label{eq:S1-utterance-salience}
  P_{S_1(u\mid s)}^{\text{Salience}}   & \propto \text{Truth}(u,s) \ \text{Informativity}(u)^{\alpha} \ \text{Salience}(u)
\end{align}
The implementation of the speaker model in \eqref{eq:S1-utterance-salience} includes the
factor $\text{Salience}(u)$ as an \emph{utterance prior}, suggesting an algorithmic
picture of utterance choice: the speaker searches for utterances to choose, based on a gradient
of how easy these utterances come to mind, then compares the available options (weighted by
their salience/prominence) based on the other factors relevant for communication: truth and
informativity. \mf{I'm not sure if this paragraph needs to go here; it might have a better
  place elsewhere; but stressing the 'algorithmic picture' somewhere might be good} \gcs{yeah, this paragraph feels a bit out of place -- I wonder if it would be possible to tie it back more closely to the existing RSA formalism}

\paragraph{Informativity from alignment of beliefs.} We saw above that the Vanilla RSA
model's definition of speaker's choice probabilities can be interpreted as a product of three
factors that correspond closely to Gricean maxims of Quality, Quantity and Manner.
This interpretation is just one formal implementation; others are readily conceivable. Given the particular
importance of informativity for many pragmatic explanations, one would be justified in further dissecting
the way informativity is treated in the RSA model.

In the following, we show that \eqref{U} and \eqref{S1} follow from a simple idea.
The speaker has some belief about the relevant world states that gets captured in a probability distribution $P_{S_{1}\text{-}Bel}$;
after hearing utterance $u$, the literal listener's beliefs about the relevant world states are
represented as another probability distribution $P_{L_{0}\text{-}Bel}(u)$. An utterance $u$ is more useful
than another utterance $u'$ to the extent that $P_{L_{0}\text{-}Bel}(u)$  (i.e., the listener's beliefs after hearing $u$) is
more closely aligned with $P_{S_{1}\text{-}Bel}$ (i.e., the speaker's beliefs) than
$P_{L_{0}\text{-}Bel}(u')$  (i.e., the listener's beliefs after hearing $u'$).
Vanilla RSA corresponds to a implementation of ``alignment'' in terms of an information-theoretic notion of divergence between probability distributions.
Behind it all, however, is simply the idea that the speaker prefers
utterances that best align speaker's and listener's relevant beliefs about the world.

A useful notion of ``alignment of probability distributions'' is the information-theoretic
measure of Kullback-Leibler divergence. KL-divergence is not symmetric, but considers one of
two to-be-compared probability distributions as the ground-truth or the objective function
to be approximated.
This asymmetry makes sense in language understanding because it is the speaker's
beliefs to which the listener should align. KL-divergence then measures divergence in terms of,
essentially, the expected number of extra bits needed to encode a signal that was generated
from the true distribution with the approximate distribution. \mf{leave out that last
  sentence?} The Kullback-Leibler divergence between (baseline/true) probability distribution
$P_{S_{1}\text{-}Bel}$ and (approximate/to-be-optimized) probability distribution
$P_{L_{0}\text{-}Bel}(u)$ is defined as:
\begin{align}
  \label{eq:KL-divergence}
  \text{KL}(P_{S_{1}\text{-}Bel} \mid \mid P_{L_{0}\text{-}Bel}(u)) = - \sum_{s} P_{S_{1}\text{-}Bel}(s) \ \log \frac{P_{S_{1}\text{-}Bel}(s)}{P_{L_{0}\text{-}Bel}(u,s)}
\end{align}

Using KL-divergence, we can then state a more general definition of utterance utilities, to
replace \eqref{U}:
\begin{align}
  \label{eq:Utils-KL-based}
  U_{S_1}(u; s) = \text{KL}(P_{S_{1}\text{-}Bel} \mid \mid P_{L_{0}\text{-}Bel}(u)) - C(u)
\end{align}
The formulation in \eqref{U} is a special case of the more general \eqref{eq:Utils-KL-based}
that follows if the speaker's beliefs about the relevant world states are degenerate, that is,
whenever the speaker knows the true world state $s^{*}$, so that $P_{S\text{-}Bel}(s^{*})=1$.
In that case, we have:
\begin{align*}
  \text{KL}(P_{S_{1}\text{-}Bel} \mid \mid P_{L_{0}\text{-}Bel}(u)) & = - \sum_{s} P_{S_{1}\text{-}Bel}(s) \ \log \frac{P_{S_{1}\text{-}Bel}(s)}{P_{L_{0}\text{-}Bel}(u,s)} \\
  & =  - \log\frac{1}{P_{L_{0}\text{-}Bel}(u,s^*)} = \log P_{L_{0}\text{-}Bel}(u,s^*)
\end{align*}


\section{Variations on Vanilla} \label{variations}

We have seen how Vanilla RSA can be used to model pragmatic reasoning in simple reference games, as well as specificity implicatures more generally. In this section, we explore recent extensions of RSA meant to handle more complex language phenomena. We organize our discussion around deviations from Vanilla RSA, highlighting novel technology in language understanding computation and the phenomena they capture. \mht{a sentence about how RSA is an active area of research, new developments are happening all the time, ...? our goal is to give the naive reader a sense of the landscape and the variants that have been considered as of this date...}

As a point of reference for Vanilla RSA, we repeat the $S_1$ utterance choice rule below, where a pragmatic speaker selects utterances in proportion to their utility in conveying some observed state of affairs to a naive listener who interprets the utterance according to its literal semantics---all while minimizing utterance cost. As we shall see, many of the extensions of RSA can be characterized in terms of their deviation from this simple utterance-choice rule.
\begin{equation} \label{S1-repeat}
P_{S_1}(u\mid s) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s\mid \sem{u}) - C(u)))
\end{equation}

\subsection{Meaning inference} \label{meaning-inference}

Vanilla RSA assumes a fixed utterance semantics that is shared by both speakers and listeners. But what happens when we parameterize the interpretation function such that the utterance semantics is subject to change? This move is illustrated in \eqref{S1-lexical-uncertainty}, where the variable \textbf{x} determines the truth-functional mapping for an utterance $u$. Once named, this interpretation-fixing variable \textbf{x} becomes subject to active pragmatic reasoning. In other words, we can allow for uncertainty around \textbf{x}, such that the precise semantics of $u$ might be vague or underspecified in context. What results is the class of lexical uncertainty RSA models. \mht{Do we want to show the pragmatic listener model too?}
\begin{equation} \label{S1-lexical-uncertainty}
P_{S_1}(u\mid s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s\mid \sem{u}^{\textbf{x}}) - C(u)))
\end{equation}

\cite{lassitergoodman2013} use this technology to model the interpretation of vague gradable adjectives, whose meaning depends crucially on properties of the context, both linguistic and extra-linguistic. Take the adjective \emph{heavy}, which is true of some state (i.e., a weight) just in case the weight exceeds the relevant threshold for heaviness. However, we conclude that a \emph{heavy elephant} weighs substantially more than a \emph{heavy backpack}; moreover, the weight we attribute to the backpack is likely to vary depending on whether the speaker is a preschooler or an olympian. Thus, the threshold above which some state counts as heavy is unlikely to be a fixed, context-invariant value.

\citeauthor{lassitergoodman2013} handle the uncertainty around the threshold value by parameterizing the meaning of utterances containing gradable adjectives: \emph{heavy} will be true of some state just in case the weight exceeds the relevant threshold (as before), where this threshold (\textbf{x} in \eqref{S1-lexical-uncertainty}) gets fixed by pragmatic reasoning:

\ex. \label{heavy-sem}
\sem{heavy}$^{\textbf{x}}$ = \lam s. weight(s) $>$ \textbf{x}

It is up to the pragmatic listener, $L_1$, to resolve the uncertainty surrounding \textbf{x}. To do so, $L_1$ hears the utterance and jointly infers both the state of the world (i.e., the weight of the state being described) and the relevant heaviness threshold. $L_1$ performs this inference just as in the vanilla model: which state and threshold would have been most likely to lead $S_1$ to select the utterance that $L_1$ encountered? In other words, $L_1$ simulates $S_1$'s behavior for the possible combinations of states and thresholds, then weights states and thresholds in proportion to the probability that they would lead $S_1$ to choose the utterance that was encountered.\mht{mention how this same mechanism is used for generics?}

This technology---parameterization of the utterance semantics subject to pragmatic reasoning---allows for a recasting of the division of labor between semantics and pragmatics. With lexical uncertainty, the semantic content of an utterance can be (at least partially) determined via pragmatic inference. \cite{bergenetal2016} seize on this innovation to model M-implicatures \citep{horn1984} and certain embedded implicatures \citep{hurford1974,chierchiaetal2012}. Rather than uncertainty around the semantics of a single utterance (e.g., a gradable adjective, as in the example above), \citeauthor{bergenetal2016} assume uncertainty around entire lexica. \gcs{need to say more about lexical uncertainty?} \mht{Frank and Goodman (2014) and Bohn, Tessler, and Frank (2019) use lexical uncertainty (or, at least in spirit for FG2014, if not exactly in their model) to model word learning via mutual-exclusivity reasoning (e.g., ``dax'' probably means novel object, and not the car).} \mht{do we / will we have a discussion somewhere about language / word learning?} \gcs{(word) learning seems like a good thing to bring up in the extensions section}

Meaning inference also serves as a useful tool for modeling ambiguity resolution. \cite{scontrasgoodman2017} use an utterance-choice rule as in \eqref{S1-lexical-uncertainty} to model the resolution of distributive-collective ambiguities in plural predication. If a listener hears that \emph{the boxes are heavy}, the listener must jointly infer both the intended interpretation of the utterance (i.e., distributive, commenting on individual box weight, or collective, commenting on the total weight of the set) and the state of the world (i.e., the weights of the boxes). Returning to \eqref{S1-lexical-uncertainty}, \citeauthor{scontrasgoodman2017} treat \textbf{x} as the interpretation-resolving variable that determines whether $u$ receives a distributive or collective interpretation. \cite{savinellietal2017,savinellietal2018} use this technology to model the resolution of scope ambiguity, as in utterances like \emph{every horse didn't jump over the fence}. For \citeauthor{savinellietal2017}, \textbf{x} in \eqref{S1-lexical-uncertainty} serves to determine the relative scope of quantification elements at LF.

Both of these applications---plural predication and quantifier scope ambiguity---highlight how meaning inference in RSA can serve as a useful shorthand for providing computational-level descriptions of the process of ambiguity resolution. However, while these models offer a promising hypothesis for how listeners reason pragmatically to resolve ambiguity, it bears noting that neither application attempts to model the compositional processes that give rise to the relevant ambiguities in the first place. We return to this issue in Section \ref{limitations}.


\subsection{QUD inference}

The technology we have considered up to now allows for the pragmatic enrichment of utterances: from ``blue'' to ``blue square'' or from vague ``expensive'' to ``expensive'' with a contextually-determined price threshold. However, given that $S_1$'s utility relies on successfully communicating the observed $s$ to $L_0$ (i.e., on informativity), our enrichments can only ever be literal---we have no way for ``blue'' to be interpreted as ``green''. However, many instances of everyday language use are, strictly speaking, non-literal. For example, if you hear that someone paid ``a million dollars'' for their coffee at the hipster hangout around the corner, you are unlikely to infer that the actual price paid was \$1,000,000. Rather, you infer that the price was high and that the speaker is, to put it mildly, less than thrilled.

To allow for enrichments beyond the literal meaning, we must broaden $S_1$'s goals beyond informativity with respect to $s$. One way of conceiving of speaker goals is in terms of the Question Under Discussion (QUD) the speaker aims to answer. Under certain theories of pragmatics, all discourse proceeds with respect to some QUD \citep{roberts2012}; utterances in the discourse must at least partially answer the QUD in order to be cooperative and felicitous. By allowing for a broader range of QUDs and uncertainty around which QUD is intended, vanilla RSA may be extended to capture uses of non-literal language.

As was the case with meaning inference in the previous subsection, we can illustrate this innovation by highlighting its effect on the vanilla utterance-choice rule. In \eqref{S1-QUD}, $S_1$'s utility continues to break down into informativity and economy components. However, now \textbf{x} determines what it is that $S_1$ endeavors to communicate to $L_0$. Viewed as the QUD, \textbf{x} serves to map $s$ to the answer to \textbf{x} that $S_1$ would like to communicate to $L_0$.
\begin{equation} \label{S1-QUD}
P_{S_1}(u\mid s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(F(s,\textbf{x})\mid \sem{u}) - C(u)))
\end{equation}

\cite{kaoetal2014} use this technology to model hyperbole, as in the coffee-price example above. To see how, consider some possible states of the world about which a speaker might want to communicate:

\ex. \label{hyperbole-states}
\emph{Possible states of the world for the coffee-price scenario}:\\
$s_1$: \{\texttt{affect}: positive, \texttt{price}: \$3\}\\
$s_2$: \{\texttt{affect}: positive, \texttt{price}: \$7\}\\
$s_3$: \{\texttt{affect}: positive, \texttt{price}: \$1,000,000\}\\
$s_4$: \{\texttt{affect}: negative, \texttt{price}: \$3\}\\
$s_5$: \{\texttt{affect}: negative, \texttt{price}: \$7\}\\
$s_6$: \{\texttt{affect}: negative, \texttt{price}: \$1,000,000\}

Notice that the world states in \ref{hyperbole-states} have two properties: the \texttt{affect} of the speaker (i.e., whether the speaker feels positive or negative about the price paid), together with the actual \texttt{price} paid. A speaker talking about $s$ might have the goal of communicating about their \texttt{affect} and the \texttt{price} paid, or they may want to communicate only their \texttt{affect} or only the \texttt{price} paid. Viewed in terms of QUDs, these goals serve to partition the state space. With an \texttt{affect?} QUD, we partition the state space into two cells, corresponding to positive vs.~negative \texttt{affect}. With a \texttt{price?} QUD, we partition the state space into three cells, corresponding to the three possible \texttt{price}s. With a QUD asking after both \texttt{affect-and-price?}, we partition the state space into the six cells listed in \ref{hyperbole-states}.

With the utterance-choice rule in \eqref{S1-QUD}, $S_1$'s goals are QUD specific: for communication to succeed, $L_0$ must simply arrive at the correct cell of the relevant partition; whatever $L_0$ infers about what happened within that partition is irrelevant to $S_1$'s goals. For example, if $S_1$ has a negative affect and is addressing the \texttt{affect?} QUD, utility depends on whether or not $L_0$ correctly arrives at the negative cell of the \texttt{affect?} partition, regardless of whether $L_0$ infers the full state to be $s_1$, $s_2$, or $s_3$. It is at this point where non-literal language suddenly becomes rational for a speaker. To see how, we have to say more about the prior knowledge speakers and listeners bring to bear on their communication scenarios.

World knowledge tells us that states in which the speaker actually paid \$1,000,000 (i.e., $s_3$ and $s_6$) are extraordinarily improbable, while states in which the coffee cost \$3 (i.e., $s_1$ and $s_4$) are the most likely; this knowledge gets reflected in $L_1$'s prior over world states, $P(s)$. We also have prior knowledge about how probable a speaker is to have a positive vs.~negative affect given a specific price paid for a cup of coffee: as prices increase, the probability of negative affect increases along with it. Now, return to a speaker addressing the \texttt{affect?} QUD. If $S_1$'s primary objective is to communicate negative affect to $L_0$, and, crucially, if the available utterances only concern price (i.e., ``I paid \$3/\$7/\$1,000,000''), then the rational thing for $S_1$ to say is that the coffee cost ``\$1,000,000''; given the extremely strong association between \$1,000,000 coffee and negative affect, $L_0$ is all but certain to arrive at the correct answer to the QUD: the speaker holds negative affect.

We have identified how non-literal utterances can be useful to a speaker, but we have yet to capture the non-literal aspect of their meaning. It is at the level of $L_1$ where utterances suddenly become non-literal. Hearing that the speaker paid ``\$1,000,000'', $L_1$ uses the informative priors on prices and their associated affects to arrive at a plausible interpretation. Given that coffee is sure to cost less than \$1,000,000, $L_1$ considers the possible QUDs the speaker might be addressing when selecting $u$. The \texttt{price?} and \texttt{price-and-affect?} QUDs are highly unlikely, given that \$1,000,000 is a highly unlikely price to pay for a cup of coffee. That leaves the \texttt{affect?} QUD. So, $L_1$ considers the possible full states that would have led $S_1$ to utter ``\$1,000,000'' in response to the \texttt{affect?} QUD. Via this counterfactual reasoning, $L_1$ arrives at a reasonable interpretation: $S_1$ is likely to have paid more than usual, so $s_1$ and $s_4$ are downweighted, but we're still on earth, so $s_3$ and $s_6$ remain highly unlikely. Of the remaining states, $S_1$ is most likely to have wanted to communicate negate affect with ``\$1,000,000'', so $L_1$ concludes that $s_5$ is more probably than $s_2$. In other words, $L_1$ hears that the speaker paid ``\$1,000,000'' for a cup of coffee and concludes 1) that the speaker is likely address the \texttt{affect?} QUD, 2) that the speaker is likely trying to communicate negative affect, and 3) that the price is higher than usual but still within the realm of possibility. This last conclusion---a hyperbolic interpretation of the price---is where non-literal meaning enters.

\cite{kaogoodman2015} use an extension of the hyperbole model to capture ironic interpretations of weather descriptions (e.g., ``It's terrible out'' to describe a beautiful sunny day). In addition to inferring speaker affect, the irony model incorporates an additional dimension of meaning that a speaker might want to communicate: their arousal about the weather (i.e., whether they feel strongly about it). The non-literal interpretation mechanism remains the same: if a speaker describes the weather as ``terrible'' but we know that terrible weather is extremely unlikely (the authors situate their imagined conversation in California, where the weather is rarely objectionable), then a listener will conclude that the unlikely literal state (i.e., terrible weather) does not hold, but rather that the speaker must be addressing some other dimension of meaning, namely the strong arousal they feel in their positive affect toward the normal state of affairs: good weather. Similarly in \cite{kaoetal2014metaphor}, who model the interpretation of metaphors like ``John is a whale''. Rather than inferring that John is an aquatic mammal (a highly unlikely state of affairs), the listener concludes that the speaker chose the utterance to communicate about some other dimension of meaning, for example John's physical stature, which has whale-like properties.

\gcs{also discuss the QUD manipulation in the quantifier-scope model?}


\subsection{Context/Prior inference}

We now have technology to model vagueness and ambiguity on the basis of an underspecified semantics, as well as shifting goals and the nonliteral language that address them on the basis of uncertainty around the QUD. So far, all of this reasoning has happened with respect to a fixed context, or common ground. What happens when a listener is unsure of the context a speaker has used in the production of their utterance? \mht{might want to draw this out as a very general problem: what aspects of context should the listener pay attention to?} For example, if you hear that some person is tall, how does a listener know what the referent is tall relative to? 

The formal mechanism by which an adjective like \emph{tall} receives a context-specific interpretation is the \emph{comparison class}, the (often implicit) set of entities or events against which the referent is compared (e.g., \emph{tall} for a person~vs.~\emph{tall} for a basketball player). 
In Section \ref{meaning-inference}, we described a model of gradable adjectives where the threshold for \emph{tall} is computed via reference to the state prior, the prior distribution over heights within some comparison class. We can make this dependence on the comparison class explicit in the model of the speaker: 
%Depending on the comparison class, the height you ultimately infer for the person could vary dramatically: tall for a basketball player means something different from tall for a person.

\begin{equation} \label{S1-context}
P_{S_1}(u\mid s, \textbf{x}) \propto \textrm{exp}(\alpha (\textrm{log}P_{L_0}(s\mid \sem{u}, \textbf{x}) - C(u)))
\end{equation}


In an analogous manner to the way we treat uncertainty in the literal meaning and uncertainty in the QUD, a listener can have uncertainty as to the relevant comparison class. In the literal listener model, the prior over world states is treated as a conditional probability distribution that depends upon the comparison class \textbf{x}: $P(s\mid \textbf{x})$.

\cite{tessler2017comparisonclass} used this parameterization to model comparison class inferences when listeners hear only an adjective applied to an individual (e.g., when describing a basketball player, ``He is tall'').
The model assumes that the listener has access to the fact that the referent is a basketball player and has a knowledge that a basketball player is a person. Then, the listener's prior distribution of comparison classes is the set of all categories to which the referent is a member, weighted by the prior probability. The prior probability of different comparison classes is an unknown theoretical construct and the original comparison class inference model only uses two comparisons classes: the subordinate category (e.g., basketball player) and a basic or superordinate category (e.g., people). The inference proceeds by imagining the conditions under which a speaker would use the adjective heard (e.g., \emph{tall}) with each of the comparison classes (e.g., \emph{for a person}~vs.~\emph{for a basketball player}). 
%\gcs{I thought your $c$ was serving as \textbf{x} in this example, and your \textbf{x} is $s$.}


An important detail to this model is that pragmatic listener use their knowledge that the referent is a member of the subordinate category (i.e., the referent is a basketball player) to inform their prior distribution over the degree. That is, whereas the literal listener's prior distribution over the degree can take either comparison class variable $c$, the pragmatic listener's prior distribution over the degree always uses the subordinate category to guide their predictions about the plausible height of the referent.

The uncertainty over the comparison class is a special case of uncertainty over contexts, for which an appropriately constructed pragmatic listener model can reason over. \cite{degen2015wonky} used a very similar model to account for the stubbornness of scalar implicatures in the face of strong prior beliefs that would have made the implicature-calculated meaning. In that study, the authors investigated listener interpretations of quantified statements involving ``Some'' in a set of stimuli with diverse prior expectations concerning the probability of the \emph{all-state}. For example, ``John threw 15 marbles into the pool. Kevin, who observed what happened, said \emph{Some of the marbles sank.}''. In this scenario, the prior probability of a marble sinking in water is close to 1; thus, one would expect \emph{a priori} that all of the marbles would sink. However, the authors observe across multiple dependent measures that participants still draw the implicature---\emph{some but not all of the marbles sank}. \citeauthor{degen2015wonky} account for this behavior using this context-inference mechanism that allows listeners to revise their prior beliefs when the utterances heard are sufficiently unexpected (e.g., by positing the world is somehow strange or \emph{wonky}).

\gcs{maybe this is also the place to discuss the generics model (in terms of a fancy prior)? or should we mention generics in the context of meaning inference?}
\mht{I would think the meaning inference section is the appropriate one. We can mention fancy prior there, and point in some way to probabilistic models of cognition more broadly as a way to approach priors...}

\subsection{Epistemic inference}

By now it should be clear that a productive way of extending the RSA framework is to take into account various sorts of uncertainty that enter into communication scenarios: uncertainty about the semantics, the goals, or the context itself. Another common source of uncertainty concerns the epistemic states of the agents who are communicating. Speakers commonly describe states of the world without total knowledge of those states. For example, a speaker might observe an empty plate on John's desk with pizza crumbs on it and describe the scenario with ``John ate some of the pizza.'' As far as the speaker knows, John could have eaten all of the pizza, but, without seeing the pizza box, the speaker lacks evidence to make this stronger claim. A hungry listener will interpret the speaker's utterance to infer just how much pizza John has eaten. Importantly, whether the speaker saw the pizza box or just the crumb-covered plate is likely to influence the listener's inference about the full state of the pizza. The mechanism involves reasoning about the knowledge that the speaker used to make his utterance (i.e., the speaker's epistemic state), and the implications that the knowledge has for likely states of the world.

We can model this inference by once again amending our speaker model. Rather than having the speaker (epistemically) access the state to be communicated directly, this access can be mediated by an observation the speaker makes (e.g., observing the crumb-covered plate vs.~the empty pizza box). On the basis of this observation, the speaker infers what the true state of the world is likely to be, and then attempts to communicate the inferred state (i.e., the state the speaker believes he is in on the basis of his observation) to $L_0$. \gcs{I wonder whether we need to distinguish observation from access} This process is illustrated in \eqref{S1-epistemic}, where $S_1$ chooses $u$ on the basis of an observation \textbf{x} by determining what the true $s$ is likely to be and then maximizing the probability that $L_1$ arrives at $s$ from $u$; the value $P(s\mid \textbf{x})$ encodes $S_1$'s beliefs about the states $s$ that could have led to \textbf{x}.

\begin{equation} \label{S1-epistemic}
P_{S_1}(u\mid \textbf{x}) \propto \textrm{exp}(\alpha \mathbb{E}_{P(s\mid \textbf{x})}(\textrm{log}P_{L_0}(s\mid \sem{u}) - C(u)))
\end{equation}


\cite{goodmanstuhlmuller2013} use this innovation to model rates of scalar implicature for the quantifier ``some.'' In the pizza example, the relevant inference concerns how likely it is for the listener to strengthen the ``some'' utterance to ``some but not all.'' \citeauthor{goodmanstuhlmuller2013} hypothesize that rates of scalar implicature should decrease as the speaker has less information that would verify whether the all-state obtains (e.g., whether John in fact ate all of the pizza); the authors present behavioral data supporting this claim. The amended RSA model offers an articulated analysis of why speaker knowledge access (and listeners' awareness of it) should have the effect it does on interpretation. 

In the case of full knowledge access, $L_1$ reasons upon hearing ``some'' that if the speaker had wanted to communicate the all-state, he could have said ``all;'' but the speaker did not utter ``all,'' so he must know that the all-state does not hold. This reasoning leads $L_1$ to reassign the probability that would have been assigned to the all-state to the other states compatible with the semantics of ``some,'' namely the some-but-not-all states. This decrease in posterior probability assigned to the all-state serves as our index of scalar implicature. With partial knowledge access, the speaker may believe he is in the all-state, but he can never know for sure; similarly, upon observing only a plate with pizza crumbs, the speaker will never know that the all-state (i.e., that all the pizza was eaten) does \emph{not} hold. $L_1$ knows this about the speaker, and therefore is aware that, when hearing ``some'' from a speaker with partial knowledge access, the speaker is less likely to be in position to know that the all-state does not hold, thus canceling the counterfactual reasoning mechanism driving scalar implicature.

In their model of distributive-collective ambiguities in plural predication, \cite{scontrasgoodman2017} use this same sort of speaker knowledge manipulation to modulate rates of distributive interpretations. Hearing ``the boxes are heavy'' from a speaker who was unable to access the weights of individual boxes---because, for example, the speaker used a dolly to move all the boxes at once, and so accessed only their collective weight---a listener is less likely to interpret the utterance distributively, as commenting on the individual box weights. Rather, the listener is more likely to assign the utterance a collective interpretation, commenting on the total weight of the set of boxes. The reasoning comes straight from \cite{Grice1975:Logic-and-Conve}: ``Do not say that for which you lack adequate evidence.'' In \citeauthor{scontrasgoodman2017}'s model, the pragmatic listener is reasoning about a speaker who lacks the knowledge to verify a distributive interpretation on the basis of the (partial) observation he has made about the state of the boxes.


\subsection{Complex utility/utility inference}
 
 So far, we have considered a host of different parameterizations to RSA that revolve around the problem of understanding meaning in context by assuming that speakers want to convey information to the listener. Though informational uses of language are arguably the bread-and-butter of a successful communicative system, speakers do not always say what they mean. We dawdle, prevaricate, and sometimes outright lie, and this is often in the service of maintaining our social relationships. Perhaps rather than telling your friend that their haircut is ugly, you could say ``it's an interesting look''. How can speakers deviate from choosing the maximally informative utterance? We can begin to model non-informational uses of language by augmenting the speaker's utility function: 
  
 \begin{equation} \label{S1-polite}
P_{S_1}(u\mid s, \textbf{x}) \propto \textrm{exp}(\alpha  (
 \textbf{x} \cdot \textrm{log}P_{L_0}(s \mid \sem{u}) + 
 (1 - \textbf{x}) \cdot  \mathbb{E}_{P_{L_0}(s \mid \sem{u})}[V(s)] - C(u)))
\end{equation}

\mht{do we want to make the speaker distribution conditional on $V$ as well?}
Now $S_1$ chooses an utterance on basis the of true state $s$ (e.g., the speaker's true feelings about the haircut), where the standard, RSA information utility is one component of the utility function. 
This informational utility is weighted by a mixture parameter $\textbf{x}$, were the second component receives the opposite weight ($1-\textbf{x}$).
The second component is what \cite{yoonetal2016} call \emph{social utility} and is a function of the literal listener's subjective value $V$ of the state of the world they believe they are in given the utterance.
\cite{yoonetal2016} deploy this generalization of the speaker utility to account for polite language use. 
It formalizes a version of \cite{brown1987politeness}'s politeness theory, in which cooperative speakers have social goals to minimize any potential damage to the hearers (and the speakers own) self-image (called \emph{face}), in addition to standard informational-goals. 
Thus, $S_1$'s utility is a mixture of two-utilities: standard epistemic utility defined in Eq.~\ref{U} and a social utility: $U_{social}(u)  =  \mathbb{E}_{L_0(s \mid \sem{u})}[V(s)]$.

One thing to note about $U_{social}(u)$ is that it is a function of only the utterance $u$ because of the expectation in the definition: It is the expected subjective value of the state the literal listener believes themselves to be in (averaging over the possible states the listener believes themselves to be in). $V$ is a value function that maps states of the world onto subjective values; in the case study explored by \cite{yoonetal2016}, the value of the state of the world was denoted explicitly in terms of a yelp-style review (\# of hearts on a ``heart-scale''), but this is area where work from cognitive science can help articulate more realistic value functions for listeners. The effect of this social utility term is that increases the speaker $S_1$'s production probabilities of utterances that convey states with high subjective value (e.g., ``it's \emph{beautiful}'', ``your talk was \emph{amazing}''), which can lead to ``white lies''. 

The pragmatic consequences of this complex utility function can be realized at the higher levels of pragmatic recursion.
The pragmatic listener $L_1$ can reason jointly about the state of the world and about the speaker's utility function (specifically, what is the speaker's trade-off between informativity vs. social goals?). \cite{yoonetal2016} showed that with information about the true state of the world and the utterance, the model can recover the speaker's likely goals, and visa versa, with information about the speaker's goals and the utterance, the model recovers the likely true state of the world.
\cite{yoonetal2017, yoonetal2018} explored the implications of this utility inference mechanism on the higher-order speaker $S_2$. Because $L_1$ reasons about the speaker's utility, $S_2$'s utility function can be modified to include a term based on the listener's inference about the utility function, what they call a ``self-presentational utility''. This term contributes positive utility to the higher-order $S_2$'s utterance choice when the utterance makes the speaker appear as if they had a particular goal in mind (e.g., a particular utility trade-off). \cite{yoonetal2018} showed how this mechanism is important for capturing polite, indirect speech, especially when the utterance has low informational utility or is literally false (e.g., saying ``your talk wasn't terrible'' when \emph{terrible} is quite likely true). 

\mht{show S2?}


\section{Modeling practicalities} \label{practicalities}

Our hope in the previous section was to present a menu of options to the would-be modeler: given a language-understanding phenomenon, which pieces of technology are likely prove most useful in a formal treatment? Non-literal phenomena might benefit from a treatment via QUD inference or complex utility; vague or underspecified language suggests meaning inference, combined perhaps with uncertainty about the relevant context/prior; in cases where the speaker is unknowledgeable or behaves in an unexpected manner, we can try complex utility and/or context inference. Having introduced the basics of the RSA modeling framework, as well as useful additions and amendments, our focus now turns to the practical considerations involved in designing and implementing an RSA model, and testing the predictions of that model against human behavior. 

One of the most important modeling decisions concerns setting up the conversational scenario: how do we represent the states of the world that can be described in our model? Answering this question entails identifying the essence of the language phenomenon of interest. In the reference game from \cite{frankgoodman2012} depicted in Figure \ref{ref-game}, we equate states with individual objects, and the set of possible states corresponds to the set of possible objects present in the visual array. But only certain features of the objects are relevant: their shape and color. Properties like size or position do not enter into the reasoning about shape and color in this reference game (or at least we don't think they do), and so they need not enter into the representation of states in the model. In \ref{hyperbole-states}, we saw possible states from the hyperbole model of \cite{kaoetal2014}; of all the various facts of the world, the authors narrowed in on two for their model: the price paid for some object, and valence of the affect of the person who paid it. But the authors also narrowed the possible prices. 

One of the more ingenious---and abstract---state implementations comes from the generics model \citep{tesslergoodman2019}. Generic statements update beliefs about the prevalence of a feature in a category (e.g., the prevalence of egg-laying among robins upon hearing that ``robins lay eggs''). The prior, then, is a distribution over possible prevalences. \citeauthor{tesslergoodman2019} treat this distribution as a distribution over alternative kinds (e.g., bluejays, horses, tarantulas, etc.), akin to the implementation of the comparison class in the vague adjectives models \citep{lassitergoodman2013, tessler2017comparisonclass}. Each kind is modeled as its prevalence for the relevant property; the resulting distributions have complex structures, reflecting abstract, intuitive theories about kinds and properties, which can also be well-modeled using the tools of probabilistic calculus \citep{tenenbaum2011grow}. 

In addition to deciding how to model the various state possibilities, one must also determine which possibilities to include. This choice is relevant to every aspect of the model where alternatives must be specified: alternative states, alternative utterances, alternative QUDs, etc. Unfortunately, the literature on RSA has not yet arrived at a set of best practices for determining alternatives (indeed, this issue is a problem for pragmatic theories more broadly). XXX \gcs{has anyone used a free response method to determine possible utterances? or QUDs? or states? We did something like this for the epistemic must model, but I don't think it ever got published.}

Having decided what the relevant alternatives are and how to model them, the task then turns to implementing the appropriate priors: beliefs about which alternatives are more or less likely a priori. Some priors can be estimated empirically. With state priors, it might be possible to ask people explicitly about the relevant probabilities. For example, \cite{kaoetal2014} asked participants to rate the probability of paying various prices for certain objects, and also how likely it is that someone would consider a given price expensive; those ratings were normalized and served as the state prior probabilities in the hyperbole model. In the reference game model, \cite{frankgoodman2012} got at the state prior probabilities using a less direct method. To determine which objects in Figure \ref{ref-game} were more likely to get referenced a priori, the authors told participants that someone had used an unknown word to signal one of the objects; based on participants' guesses at which object the speaker was intending, \citeauthor{frankgoodman2012} were able to estimate the relevant state prior. In the generics model, \cite{tesslergoodman2019} \gcs{how you estimated the state prior using targeting questions to determine parameters of the relevant distributions}.

In some cases---for example, an analytic demonstration of a qualitative prediction---prior estimation might be unnecessary or infeasible. In such cases, one ought to make minimal assumptions, for example by assuming an uninformative, uniform prior. Uniform distributions are often used in the implementation of the prior over possible utterances. \gcs{something about the philosophy behind a uniform utterance prior?}

Like the specific probabilities assigned in the various priors, other free parameters of the model must be fixed in order to generate predictions. Free parameters common to RSA include $\alpha$, which controls speaker optimality, and the various utterance cost parameters. \gcs{a note on BDA?}

With values for the various priors and free parameters, it is possible to generate predictions from an RSA model. However, interpreting those predictions requires a clear linking function from the model output to human behavior, and the specific linking function needed will depend on the human behavior to be modeled. 


\section{Extensions/limitations} \label{limitations}

* order doesn't matter

\mf{}\mht{} Incrementality / processing

\gcs{}Compositionality (CCG chapter in dippl?)

\mf{}NLP (Dan Klein)

\mf{}Individual variability (cite Franke \& Degen)

\mf{}\gcs{}``Just'' a computational level theory. Doesn't bear on mechanisms. But, potentially a hook via resource rational analysis (Leider, Griffiths, ...)

\mf{}Hand-coded: Don't have a theory of alternatives, state priors

\section{Summary and outlook} \label{summary}

XXX


\bibliography{problang}

%\begin{addresses} %%% uncomment to de-anonymize
%	\begin{address}
%		author1
%		\email{email1}
%	\end{address}
%	\begin{address}
%		author2
%		\email{email2}
%	\end{address}
%	\begin{address}
%		author3
%		\email{email3}
%	\end{address}
%\end{addresses}



\end{document}
